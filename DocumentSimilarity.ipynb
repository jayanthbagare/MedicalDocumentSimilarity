{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPa7+IEEOYAp9ak8TKAeJSW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jayanthbagare/MedicalDocumentSimilarity/blob/main/DocumentSimilarity.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "-gkCTmNCYxpq"
      },
      "outputs": [],
      "source": [
        "# Required libraries\n",
        "# pip install sentence-transformers scikit-learn nltk spacy textstat numpy scipy\n",
        "\n",
        "# Download required models (run once)\n",
        "# python -m spacy download en_core_web_sm\n",
        "\n",
        "import nltk\n",
        "# Run these once to download required data\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import spacy\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import textstat\n",
        "import re\n",
        "\n",
        "class DocumentAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.nlp = spacy.load(\"en_core_web_sm\")\n",
        "        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "    def preprocess_document(self, text):\n",
        "        \"\"\"\n",
        "        Preprocesses a document into its component parts\n",
        "\n",
        "        Args:\n",
        "            text (str): Raw document text\n",
        "\n",
        "        Returns:\n",
        "            dict: Contains raw_text, sentences, paragraphs, and spacy doc object\n",
        "        \"\"\"\n",
        "        # Clean and segment the document using spaCy\n",
        "        doc = self.nlp(text)\n",
        "\n",
        "        # Extract sentences using spaCy's sentence segmentation\n",
        "        sentences = [sent.text.strip() for sent in doc.sents]\n",
        "\n",
        "        # Extract paragraphs by splitting on double newlines\n",
        "        # Filter out empty paragraphs\n",
        "        paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
        "\n",
        "        # Handle case where document has no paragraph breaks\n",
        "        if len(paragraphs) == 0:\n",
        "            paragraphs = [text.strip()]\n",
        "\n",
        "        return {\n",
        "            'raw_text': text,\n",
        "            'sentences': sentences,\n",
        "            'paragraphs': paragraphs,\n",
        "            'doc_object': doc\n",
        "        }"
      ],
      "metadata": {
        "id": "Ko30m2AZY_Sm"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SemanticAnalyzer:\n",
        "    def __init__(self, sentence_model):\n",
        "        self.sentence_model = sentence_model\n",
        "\n",
        "    def document_level_similarity(self, doc1, doc2):\n",
        "        \"\"\"\n",
        "        Computes overall semantic similarity between two documents\n",
        "\n",
        "        Args:\n",
        "            doc1, doc2: Preprocessed document dictionaries\n",
        "\n",
        "        Returns:\n",
        "            float: Cosine similarity score between document embeddings (0-1)\n",
        "        \"\"\"\n",
        "        # Encode entire documents as single embeddings\n",
        "        emb1 = self.sentence_model.encode([doc1['raw_text']])\n",
        "        emb2 = self.sentence_model.encode([doc2['raw_text']])\n",
        "\n",
        "        # Calculate cosine similarity\n",
        "        return cosine_similarity(emb1, emb2)[0][0]\n",
        "\n",
        "    def paragraph_level_similarity(self, doc1, doc2):\n",
        "        \"\"\"\n",
        "        Aligns paragraphs between documents to find best semantic matches\n",
        "\n",
        "        Uses Hungarian algorithm to find optimal paragraph-to-paragraph alignment\n",
        "\n",
        "        Returns:\n",
        "            dict: Contains average alignment score and individual alignment pairs\n",
        "        \"\"\"\n",
        "        # Handle empty paragraph lists\n",
        "        if not doc1['paragraphs'] or not doc2['paragraphs']:\n",
        "            return {\n",
        "                'average_alignment_score': 0.0,\n",
        "                'alignment_pairs': []\n",
        "            }\n",
        "\n",
        "        # Generate embeddings for all paragraphs\n",
        "        p1_embeddings = self.sentence_model.encode(doc1['paragraphs'])\n",
        "        p2_embeddings = self.sentence_model.encode(doc2['paragraphs'])\n",
        "\n",
        "        # Create similarity matrix (each cell is cosine similarity)\n",
        "        similarity_matrix = cosine_similarity(p1_embeddings, p2_embeddings)\n",
        "\n",
        "        # Find optimal alignment using Hungarian algorithm\n",
        "        # We negate the matrix because linear_sum_assignment finds minimum cost\n",
        "        from scipy.optimize import linear_sum_assignment\n",
        "        row_indices, col_indices = linear_sum_assignment(-similarity_matrix)\n",
        "\n",
        "        # Extract the similarity scores for the optimal alignment\n",
        "        aligned_scores = similarity_matrix[row_indices, col_indices]\n",
        "\n",
        "        return {\n",
        "            'average_alignment_score': np.mean(aligned_scores),\n",
        "            'alignment_pairs': list(zip(row_indices, col_indices, aligned_scores)),\n",
        "            'similarity_matrix': similarity_matrix\n",
        "        }\n",
        "\n",
        "    def sentence_level_patterns(self, doc1, doc2):\n",
        "        \"\"\"\n",
        "        Compares the overall semantic \"style\" of sentences between documents\n",
        "\n",
        "        Method: Computes mean embedding for all sentences in each document,\n",
        "        then compares these mean embeddings\n",
        "\n",
        "        Returns:\n",
        "            float: Similarity of sentence-level semantic patterns\n",
        "        \"\"\"\n",
        "        # Handle empty sentence lists\n",
        "        if not doc1['sentences'] or not doc2['sentences']:\n",
        "            return 0.0\n",
        "\n",
        "        # Generate embeddings for all sentences\n",
        "        s1_embeddings = self.sentence_model.encode(doc1['sentences'])\n",
        "        s2_embeddings = self.sentence_model.encode(doc2['sentences'])\n",
        "\n",
        "        # Compute centroid (mean) of sentence embeddings for each document\n",
        "        s1_mean = np.mean(s1_embeddings, axis=0)\n",
        "        s2_mean = np.mean(s2_embeddings, axis=0)\n",
        "\n",
        "        # Compare the centroids\n",
        "        return cosine_similarity([s1_mean], [s2_mean])[0][0]"
      ],
      "metadata": {
        "id": "dFSvV3IVZIlZ"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class StructuralAnalyzer:\n",
        "    def __init__(self, nlp_model):\n",
        "        self.nlp = nlp_model\n",
        "\n",
        "class StructuralAnalyzer:\n",
        "    def __init__(self, nlp_model):\n",
        "        self.nlp = nlp_model\n",
        "\n",
        "    def document_organization_similarity(self, doc1, doc2):\n",
        "        \"\"\"\n",
        "        Compare high-level document structure metrics\n",
        "\n",
        "        Analyzes:\n",
        "        - Number of paragraphs and sentences\n",
        "        - Average lengths\n",
        "        - Length distributions\n",
        "        \"\"\"\n",
        "        features = {}\n",
        "\n",
        "        for name, doc in [('doc1', doc1), ('doc2', doc2)]:\n",
        "            # Calculate basic structural metrics\n",
        "            paragraph_lengths = [len(p.split()) for p in doc['paragraphs']]\n",
        "            sentence_lengths = [len(s.split()) for s in doc['sentences']]\n",
        "\n",
        "            features[name] = {\n",
        "                'paragraph_count': len(doc['paragraphs']),\n",
        "                'sentence_count': len(doc['sentences']),\n",
        "                'avg_paragraph_length': np.mean(paragraph_lengths) if paragraph_lengths else 0,\n",
        "                'avg_sentence_length': np.mean(sentence_lengths) if sentence_lengths else 0,\n",
        "                'paragraph_length_distribution': paragraph_lengths\n",
        "            }\n",
        "\n",
        "        # Calculate structural similarity\n",
        "        structure_score = self._compare_structural_features(features['doc1'], features['doc2'])\n",
        "        return structure_score\n",
        "\n",
        "    def discourse_pattern_similarity(self, doc1, doc2):\n",
        "        \"\"\"Analyze discourse markers and transitions\"\"\"\n",
        "        discourse_markers = [\n",
        "            'however', 'therefore', 'furthermore', 'moreover', 'nevertheless',\n",
        "            'consequently', 'additionally', 'meanwhile', 'similarly', 'conversely',\n",
        "            'in contrast', 'on the other hand', 'as a result', 'for example',\n",
        "            'in conclusion', 'to summarize', 'first', 'second', 'finally'\n",
        "        ]\n",
        "\n",
        "        def extract_discourse_features(text):\n",
        "            text_lower = text.lower()\n",
        "            marker_counts = {marker: text_lower.count(marker) for marker in discourse_markers}\n",
        "            total_markers = sum(marker_counts.values())\n",
        "\n",
        "            return {\n",
        "                'total_discourse_markers': total_markers,\n",
        "                'marker_density': total_markers / len(text.split()),\n",
        "                'marker_distribution': marker_counts\n",
        "            }\n",
        "\n",
        "        d1_features = extract_discourse_features(doc1['raw_text'])\n",
        "        d2_features = extract_discourse_features(doc2['raw_text'])\n",
        "\n",
        "        return self._compare_discourse_features(d1_features, d2_features)\n",
        "\n",
        "    def syntactic_pattern_similarity(self, doc1, doc2):\n",
        "        \"\"\"Compare syntactic structures using POS patterns\"\"\"\n",
        "        def extract_pos_patterns(doc_object):\n",
        "            # Extract POS tag sequences for sentences\n",
        "            pos_patterns = []\n",
        "            for sent in doc_object.sents:\n",
        "                pos_sequence = [token.pos_ for token in sent if not token.is_space]\n",
        "                pos_patterns.append(tuple(pos_sequence))\n",
        "\n",
        "            # Count pattern frequencies\n",
        "            from collections import Counter\n",
        "            pattern_counts = Counter(pos_patterns)\n",
        "\n",
        "            return {\n",
        "                'pos_patterns': pattern_counts,\n",
        "                'avg_sentence_complexity': np.mean([len(p) for p in pos_patterns]),\n",
        "                'unique_patterns': len(pattern_counts)\n",
        "            }\n",
        "\n",
        "        p1 = extract_pos_patterns(doc1['doc_object'])\n",
        "        p2 = extract_pos_patterns(doc2['doc_object'])\n",
        "\n",
        "        return self._compare_syntactic_features(p1, p2)\n",
        "\n",
        "    def paragraph_role_similarity(self, doc1, doc2):\n",
        "        \"\"\"Identify and compare paragraph functional roles\"\"\"\n",
        "        def classify_paragraph_role(paragraph):\n",
        "            paragraph_lower = paragraph.lower()\n",
        "\n",
        "            # Simple heuristic-based classification\n",
        "            if any(word in paragraph_lower for word in ['introduction', 'begin', 'start', 'overview']):\n",
        "                return 'introduction'\n",
        "            elif any(word in paragraph_lower for word in ['conclusion', 'summary', 'end', 'finally']):\n",
        "                return 'conclusion'\n",
        "            elif any(word in paragraph_lower for word in ['example', 'instance', 'case study']):\n",
        "                return 'example'\n",
        "            elif any(word in paragraph_lower for word in ['argument', 'claim', 'assert', 'maintain']):\n",
        "                return 'argument'\n",
        "            else:\n",
        "                return 'body'\n",
        "\n",
        "        d1_roles = [classify_paragraph_role(p) for p in doc1['paragraphs']]\n",
        "        d2_roles = [classify_paragraph_role(p) for p in doc2['paragraphs']]\n",
        "\n",
        "        # Compare role sequences\n",
        "        from difflib import SequenceMatcher\n",
        "        role_similarity = SequenceMatcher(None, d1_roles, d2_roles).ratio()\n",
        "\n",
        "        return {\n",
        "            'role_sequence_similarity': role_similarity,\n",
        "            'doc1_structure': d1_roles,\n",
        "            'doc2_structure': d2_roles\n",
        "        }\n",
        "\n",
        "    def _compare_structural_features(self, f1, f2):\n",
        "        \"\"\"\n",
        "        Compare numerical structural features between two documents\n",
        "\n",
        "        Method:\n",
        "        1. Normalize differences between basic metrics (paragraphs, sentences, lengths)\n",
        "        2. Compare paragraph length distributions using Jensen-Shannon divergence\n",
        "        3. Combine with weighted average\n",
        "        \"\"\"\n",
        "        # Avoid division by zero errors\n",
        "        def safe_relative_diff(a, b):\n",
        "            if max(a, b) == 0:\n",
        "                return 0.0\n",
        "            return abs(a - b) / max(a, b)\n",
        "\n",
        "        # Calculate normalized differences (lower = more similar)\n",
        "        para_diff = safe_relative_diff(f1['paragraph_count'], f2['paragraph_count'])\n",
        "        sent_diff = safe_relative_diff(f1['sentence_count'], f2['sentence_count'])\n",
        "        avg_para_diff = safe_relative_diff(f1['avg_paragraph_length'], f2['avg_paragraph_length'])\n",
        "        avg_sent_diff = safe_relative_diff(f1['avg_sentence_length'], f2['avg_sentence_length'])\n",
        "\n",
        "        # Compare paragraph length distributions\n",
        "        distribution_similarity = 0.5  # Default if no valid distributions\n",
        "\n",
        "        if f1['paragraph_length_distribution'] and f2['paragraph_length_distribution']:\n",
        "            from scipy.spatial.distance import jensenshannon\n",
        "\n",
        "            # Create histograms with same bins\n",
        "            all_lengths = f1['paragraph_length_distribution'] + f2['paragraph_length_distribution']\n",
        "            if all_lengths:\n",
        "                max_length = max(all_lengths)\n",
        "                min_length = min(all_lengths)\n",
        "\n",
        "                if max_length > min_length:\n",
        "                    # Create histogram bins\n",
        "                    bins = np.linspace(min_length, max_length, 10)\n",
        "\n",
        "                    # Calculate histograms\n",
        "                    hist1, _ = np.histogram(f1['paragraph_length_distribution'], bins=bins)\n",
        "                    hist2, _ = np.histogram(f2['paragraph_length_distribution'], bins=bins)\n",
        "\n",
        "                    # Normalize to probabilities\n",
        "                    hist1_norm = hist1 / np.sum(hist1) if np.sum(hist1) > 0 else np.ones_like(hist1) / len(hist1)\n",
        "                    hist2_norm = hist2 / np.sum(hist2) if np.sum(hist2) > 0 else np.ones_like(hist2) / len(hist2)\n",
        "\n",
        "                    # Calculate Jensen-Shannon divergence (0 = identical, 1 = completely different)\n",
        "                    js_distance = jensenshannon(hist1_norm, hist2_norm)\n",
        "                    distribution_similarity = 1 - js_distance\n",
        "\n",
        "        # Combine all features with equal weighting\n",
        "        # Convert differences to similarities (1 - difference)\n",
        "        structure_score = (\n",
        "            (1 - para_diff) * 0.2 +\n",
        "            (1 - sent_diff) * 0.2 +\n",
        "            (1 - avg_para_diff) * 0.2 +\n",
        "            (1 - avg_sent_diff) * 0.2 +\n",
        "            distribution_similarity * 0.2\n",
        "        )\n",
        "\n",
        "        return max(0.0, min(1.0, structure_score))  # Clamp to [0,1]\n",
        "\n",
        "    def _compare_discourse_features(self, d1, d2):\n",
        "        \"\"\"\n",
        "        Compare discourse marker usage between documents\n",
        "\n",
        "        Args:\n",
        "            d1, d2: Discourse feature dictionaries\n",
        "\n",
        "        Returns:\n",
        "            float: Combined similarity score for discourse patterns\n",
        "        \"\"\"\n",
        "        # Compare marker density (markers per word)\n",
        "        max_density = max(d1['marker_density'], d2['marker_density'], 0.001)  # Avoid division by zero\n",
        "        density_diff = abs(d1['marker_density'] - d2['marker_density'])\n",
        "        density_similarity = 1 - min(density_diff / max_density, 1.0)\n",
        "\n",
        "        # Compare distribution of specific markers\n",
        "        all_markers = set(d1['marker_distribution'].keys()) | set(d2['marker_distribution'].keys())\n",
        "\n",
        "        if not all_markers:\n",
        "            distribution_sim = 1.0  # Both have no markers\n",
        "        else:\n",
        "            # Create vectors for all markers\n",
        "            d1_vector = [d1['marker_distribution'].get(marker, 0) for marker in all_markers]\n",
        "            d2_vector = [d2['marker_distribution'].get(marker, 0) for marker in all_markers]\n",
        "\n",
        "            # Calculate cosine similarity if both have markers\n",
        "            d1_sum = sum(d1_vector)\n",
        "            d2_sum = sum(d2_vector)\n",
        "\n",
        "            if d1_sum == 0 and d2_sum == 0:\n",
        "                distribution_sim = 1.0  # Both have no markers\n",
        "            elif d1_sum == 0 or d2_sum == 0:\n",
        "                distribution_sim = 0.0  # One has markers, other doesn't\n",
        "            else:\n",
        "                distribution_sim = cosine_similarity([d1_vector], [d2_vector])[0][0]\n",
        "\n",
        "        return (density_similarity + distribution_sim) / 2\n",
        "\n",
        "    def _compare_syntactic_features(self, p1, p2):\n",
        "        \"\"\"\n",
        "        Compare syntactic patterns between documents\n",
        "\n",
        "        Args:\n",
        "            p1, p2: Dictionaries with POS pattern features\n",
        "\n",
        "        Returns:\n",
        "            float: Combined similarity score for syntactic patterns\n",
        "        \"\"\"\n",
        "        # Compare sentence complexity (average POS tags per sentence)\n",
        "        max_complexity = max(p1['avg_sentence_complexity'], p2['avg_sentence_complexity'], 1.0)\n",
        "        complexity_diff = abs(p1['avg_sentence_complexity'] - p2['avg_sentence_complexity'])\n",
        "        complexity_similarity = 1 - min(complexity_diff / max_complexity, 1.0)\n",
        "\n",
        "        # Compare POS pattern overlap (Jaccard similarity)\n",
        "        set1 = set(p1['pos_patterns'].keys())\n",
        "        set2 = set(p2['pos_patterns'].keys())\n",
        "\n",
        "        if not set1 and not set2:\n",
        "            pattern_overlap = 1.0  # Both have no patterns\n",
        "        elif not set1 or not set2:\n",
        "            pattern_overlap = 0.0  # One has patterns, other doesn't\n",
        "        else:\n",
        "            intersection = len(set1 & set2)\n",
        "            union = len(set1 | set2)\n",
        "            pattern_overlap = intersection / union  # Jaccard similarity\n",
        "\n",
        "        return (complexity_similarity + pattern_overlap) / 2"
      ],
      "metadata": {
        "id": "o3c8_l2FZJjj"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CombinedSimilarityAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.doc_analyzer = DocumentAnalyzer()\n",
        "        self.semantic_analyzer = SemanticAnalyzer(self.doc_analyzer.sentence_model)\n",
        "        self.structural_analyzer = StructuralAnalyzer(self.doc_analyzer.nlp)\n",
        "\n",
        "    def analyze_similarity(self, text1, text2, weights=None):\n",
        "        \"\"\"\n",
        "        Comprehensive similarity analysis combining semantic and structural measures\n",
        "\n",
        "        Args:\n",
        "            text1, text2: Input documents as strings\n",
        "            weights: Dict with keys 'semantic', 'structural' (default: 60% semantic, 40% structural)\n",
        "\n",
        "        Returns:\n",
        "            dict: Comprehensive similarity analysis results\n",
        "        \"\"\"\n",
        "        if weights is None:\n",
        "            weights = {'semantic': 0.6, 'structural': 0.4}\n",
        "\n",
        "        # Validate weights\n",
        "        if not (0.99 <= sum(weights.values()) <= 1.01):  # Allow small floating point errors\n",
        "            raise ValueError(\"Weights must sum to 1.0\")\n",
        "\n",
        "        # Preprocess documents\n",
        "        print(\"Preprocessing documents...\")\n",
        "        doc1 = self.doc_analyzer.preprocess_document(text1)\n",
        "        doc2 = self.doc_analyzer.preprocess_document(text2)\n",
        "\n",
        "        print(f\"Doc1: {len(doc1['paragraphs'])} paragraphs, {len(doc1['sentences'])} sentences\")\n",
        "        print(f\"Doc2: {len(doc2['paragraphs'])} paragraphs, {len(doc2['sentences'])} sentences\")\n",
        "\n",
        "        # Semantic Analysis\n",
        "        print(\"Analyzing semantic similarity...\")\n",
        "        semantic_scores = {\n",
        "            'document_similarity': self.semantic_analyzer.document_level_similarity(doc1, doc2),\n",
        "            'paragraph_alignment': self.semantic_analyzer.paragraph_level_similarity(doc1, doc2),\n",
        "            'sentence_patterns': self.semantic_analyzer.sentence_level_patterns(doc1, doc2)\n",
        "        }\n",
        "\n",
        "        # Structural Analysis\n",
        "        print(\"Analyzing structural similarity...\")\n",
        "        structural_scores = {\n",
        "            'organization': self.structural_analyzer.document_organization_similarity(doc1, doc2),\n",
        "            'discourse_patterns': self.structural_analyzer.discourse_pattern_similarity(doc1, doc2),\n",
        "            'syntactic_patterns': self.structural_analyzer.syntactic_pattern_similarity(doc1, doc2),\n",
        "            'paragraph_roles': self.structural_analyzer.paragraph_role_similarity(doc1, doc2)\n",
        "        }\n",
        "\n",
        "        # Calculate weighted final scores\n",
        "        semantic_final = (\n",
        "            semantic_scores['document_similarity'] * 0.5 +\n",
        "            semantic_scores['paragraph_alignment']['average_alignment_score'] * 0.3 +\n",
        "            semantic_scores['sentence_patterns'] * 0.2\n",
        "        )\n",
        "\n",
        "        structural_final = (\n",
        "            structural_scores['organization'] * 0.3 +\n",
        "            structural_scores['discourse_patterns'] * 0.25 +\n",
        "            structural_scores['syntactic_patterns'] * 0.25 +\n",
        "            structural_scores['paragraph_roles']['role_sequence_similarity'] * 0.2\n",
        "        )\n",
        "\n",
        "        # Final combined score\n",
        "        overall_similarity = (\n",
        "            semantic_final * weights['semantic'] +\n",
        "            structural_final * weights['structural']\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'overall_similarity': overall_similarity,\n",
        "            'semantic_score': semantic_final,\n",
        "            'structural_score': structural_final,\n",
        "            'detailed_scores': {\n",
        "                'semantic': semantic_scores,\n",
        "                'structural': structural_scores\n",
        "            },\n",
        "            'document_stats': {\n",
        "                'doc1': {\n",
        "                    'paragraphs': len(doc1['paragraphs']),\n",
        "                    'sentences': len(doc1['sentences']),\n",
        "                    'words': len(doc1['raw_text'].split())\n",
        "                },\n",
        "                'doc2': {\n",
        "                    'paragraphs': len(doc2['paragraphs']),\n",
        "                    'sentences': len(doc2['sentences']),\n",
        "                    'words': len(doc2['raw_text'].split())\n",
        "                }\n",
        "            }\n",
        "        }"
      ],
      "metadata": {
        "id": "S5CeU-KCZMBd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize analyzer\n",
        "analyzer = CombinedSimilarityAnalyzer()\n",
        "\n",
        "# Example documents (you can also load from files)\n",
        "doc1_text = \"\"\"\n",
        "Introduction to Machine Learning\n",
        "\n",
        "Machine learning is a subset of artificial intelligence that focuses on algorithms and statistical models. These systems can automatically improve their performance on a specific task through experience.\n",
        "\n",
        "The field encompasses various approaches including supervised learning, unsupervised learning, and reinforcement learning. Each approach addresses different types of problems and data scenarios.\n",
        "\n",
        "Applications of machine learning are widespread, from recommendation systems to autonomous vehicles. The technology continues to evolve rapidly with new techniques emerging regularly.\n",
        "\"\"\"\n",
        "\n",
        "doc2_text = \"\"\"\n",
        "Understanding Machine Learning Fundamentals\n",
        "\n",
        "Machine learning represents a branch of AI that emphasizes the development of algorithms capable of learning from data. These systems enhance their performance automatically as they process more information.\n",
        "\n",
        "There are several key methodologies in this field: supervised techniques, unsupervised methods, and reinforcement-based approaches. Each methodology serves distinct problem domains and data types.\n",
        "\n",
        "Machine learning applications span numerous industries, including e-commerce recommendations and self-driving cars. The field advances quickly with continuous innovation in methodologies.\n",
        "\"\"\"\n",
        "\n",
        "# Analyze similarity with default weights (60% semantic, 40% structural)\n",
        "results = analyzer.analyze_similarity(doc1_text, doc2_text)\n",
        "\n",
        "# Print results\n",
        "print(f\"Overall Similarity: {results['overall_similarity']:.3f}\")\n",
        "print(f\"Semantic Score: {results['semantic_score']:.3f}\")\n",
        "print(f\"Structural Score: {results['structural_score']:.3f}\")\n",
        "\n",
        "# Detailed breakdown\n",
        "print(\"\\nDetailed Analysis:\")\n",
        "semantic = results['detailed_scores']['semantic']\n",
        "structural = results['detailed_scores']['structural']\n",
        "\n",
        "print(f\"Document-level semantic: {semantic['document_similarity']:.3f}\")\n",
        "print(f\"Paragraph alignment: {semantic['paragraph_alignment']['average_alignment_score']:.3f}\")\n",
        "print(f\"Sentence patterns: {semantic['sentence_patterns']:.3f}\")\n",
        "print(f\"Organization structure: {structural['organization']:.3f}\")\n",
        "print(f\"Discourse patterns: {structural['discourse_patterns']:.3f}\")\n",
        "print(f\"Syntactic patterns: {structural['syntactic_patterns']:.3f}\")\n",
        "print(f\"Paragraph roles: {structural['paragraph_roles']['role_sequence_similarity']:.3f}\")\n",
        "\n",
        "# Document statistics\n",
        "print(f\"\\nDocument Statistics:\")\n",
        "stats = results['document_stats']\n",
        "print(f\"Doc1: {stats['doc1']['paragraphs']} paragraphs, {stats['doc1']['sentences']} sentences, {stats['doc1']['words']} words\")\n",
        "print(f\"Doc2: {stats['doc2']['paragraphs']} paragraphs, {stats['doc2']['sentences']} sentences, {stats['doc2']['words']} words\")\n",
        "\n",
        "# Try different weights - emphasize structure more\n",
        "print(f\"\\nWith different weights (40% semantic, 60% structural):\")\n",
        "results_structural = analyzer.analyze_similarity(doc1_text, doc2_text,\n",
        "                                               weights={'semantic': 0.4, 'structural': 0.6})\n",
        "print(f\"Overall Similarity: {results_structural['overall_similarity']:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kb_VaLxJZS-m",
        "outputId": "77aee95b-f0d0-4256-9cf2-22da4e9a0dd1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing documents...\n",
            "Doc1: 4 paragraphs, 6 sentences\n",
            "Doc2: 4 paragraphs, 6 sentences\n",
            "Analyzing semantic similarity...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing structural similarity...\n",
            "Overall Similarity: 0.835\n",
            "Semantic Score: 0.862\n",
            "Structural Score: 0.796\n",
            "\n",
            "Detailed Analysis:\n",
            "Document-level semantic: 0.882\n",
            "Paragraph alignment: 0.799\n",
            "Sentence patterns: 0.903\n",
            "Organization structure: 0.934\n",
            "Discourse patterns: 1.000\n",
            "Syntactic patterns: 0.463\n",
            "Paragraph roles: 0.750\n",
            "\n",
            "Document Statistics:\n",
            "Doc1: 4 paragraphs, 6 sentences, 78 words\n",
            "Doc2: 4 paragraphs, 6 sentences, 78 words\n",
            "\n",
            "With different weights (40% semantic, 60% structural):\n",
            "Preprocessing documents...\n",
            "Doc1: 4 paragraphs, 6 sentences\n",
            "Doc2: 4 paragraphs, 6 sentences\n",
            "Analyzing semantic similarity...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n",
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Analyzing structural similarity...\n",
            "Overall Similarity: 0.822\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import LatentDirichletAllocation\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "def topic_structure_similarity(doc1, doc2, n_topics=5):\n",
        "    \"\"\"Compare topic distributions\"\"\"\n",
        "    combined_paragraphs = doc1['paragraphs'] + doc2['paragraphs']\n",
        "\n",
        "    vectorizer = CountVectorizer(max_features=100, stop_words='english')\n",
        "    doc_term_matrix = vectorizer.fit_transform(combined_paragraphs)\n",
        "\n",
        "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
        "    topic_distributions = lda.fit_transform(doc_term_matrix)\n",
        "\n",
        "    # Split back into documents\n",
        "    doc1_topics = topic_distributions[:len(doc1['paragraphs'])]\n",
        "    doc2_topics = topic_distributions[len(doc1['paragraphs']):]\n",
        "\n",
        "    # Compare topic progression patterns\n",
        "    d1_progression = np.mean(doc1_topics, axis=0)\n",
        "    d2_progression = np.mean(doc2_topics, axis=0)\n",
        "\n",
        "    return cosine_similarity([d1_progression], [d2_progression])[0][0]"
      ],
      "metadata": {
        "id": "0BZIvRuCZVsY"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def rhetorical_structure_similarity(doc1, doc2):\n",
        "    \"\"\"Analyze argument flow and rhetorical patterns\"\"\"\n",
        "\n",
        "    def extract_rhetorical_features(paragraphs):\n",
        "        features = {\n",
        "            'question_density': 0,\n",
        "            'claim_density': 0,\n",
        "            'evidence_density': 0,\n",
        "            'transition_density': 0\n",
        "        }\n",
        "\n",
        "        total_sentences = 0\n",
        "        for para in paragraphs:\n",
        "            sentences = nltk.sent_tokenize(para)\n",
        "            total_sentences += len(sentences)\n",
        "\n",
        "            for sent in sentences:\n",
        "                sent_lower = sent.lower()\n",
        "                if '?' in sent:\n",
        "                    features['question_density'] += 1\n",
        "                if any(word in sent_lower for word in ['argue', 'claim', 'assert', 'contend']):\n",
        "                    features['claim_density'] += 1\n",
        "                if any(word in sent_lower for word in ['evidence', 'data', 'study', 'research']):\n",
        "                    features['evidence_density'] += 1\n",
        "                if any(word in sent_lower for word in ['however', 'therefore', 'thus', 'moreover']):\n",
        "                    features['transition_density'] += 1\n",
        "\n",
        "        # Normalize by sentence count\n",
        "        for key in features:\n",
        "            features[key] = features[key] / total_sentences if total_sentences > 0 else 0\n",
        "\n",
        "        return features\n",
        "\n",
        "    f1 = extract_rhetorical_features(doc1['paragraphs'])\n",
        "    f2 = extract_rhetorical_features(doc2['paragraphs'])\n",
        "\n",
        "    # Calculate feature similarity\n",
        "    similarities = []\n",
        "    for key in f1:\n",
        "        if f1[key] + f2[key] > 0:\n",
        "            sim = 1 - abs(f1[key] - f2[key]) / max(f1[key], f2[key])\n",
        "            similarities.append(sim)\n",
        "\n",
        "    return np.mean(similarities) if similarities else 0"
      ],
      "metadata": {
        "id": "PjrmETb8ZWnl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Similarity Score Ranges:\n",
        "\n",
        "0.9-1.0: Nearly identical documents \\\n",
        "0.7-0.9: High similarity (same topic, similar approach) \\\n",
        "0.5-0.7: Moderate similarity (related content, different presentation) \\\n",
        "0.3-0.5: Low similarity (some overlap, mostly different) \\\n",
        "0.0-0.3: Very different documents \\"
      ],
      "metadata": {
        "id": "iir__uLeZczL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from typing import Dict, List, Tuple, Any\n",
        "import json\n",
        "from dataclasses import dataclass\n",
        "from datetime import datetime\n",
        "\n",
        "# You'll need to install these packages:\n",
        "# pip install sentence-transformers nltk scikit-learn spacy\n",
        "# python -m spacy download en_core_web_sm\n",
        "\n",
        "try:\n",
        "    from sentence_transformers import SentenceTransformer\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.metrics.pairwise import cosine_similarity\n",
        "    import nltk\n",
        "    import spacy\n",
        "\n",
        "    # Download required NLTK data\n",
        "    try:\n",
        "        nltk.data.find('tokenizers/punkt')\n",
        "    except LookupError:\n",
        "        nltk.download('punkt')\n",
        "\n",
        "    # Load spaCy model for medical entity extraction\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    # Load sentence transformer model\n",
        "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "except ImportError as e:\n",
        "    print(f\"Please install required packages: {e}\")\n",
        "    print(\"Run: pip install sentence-transformers nltk scikit-learn spacy\")\n",
        "    print(\"Then: python -m spacy download en_core_web_sm\")\n",
        "\n",
        "@dataclass\n",
        "class EvaluationResult:\n",
        "    semantic_similarity: float\n",
        "    bleu_score: float\n",
        "    rouge_l_score: float\n",
        "    jaccard_similarity: float\n",
        "    medical_entity_overlap: float\n",
        "    factual_consistency_score: float\n",
        "    completeness_score: float\n",
        "    clinical_relevance_score: float\n",
        "    coherence_score: float\n",
        "    overall_score: float\n",
        "    detailed_analysis: Dict[str, Any]\n",
        "\n",
        "class MedicalNoteEvaluator:\n",
        "    def __init__(self):\n",
        "        self.medical_patterns = {\n",
        "            'vital_signs': r'(?:BP|blood pressure|pulse|heart rate|temperature|temp|respiratory rate|O2 sat|oxygen saturation)[:\\s]*(\\d+[/\\-\\d]*)',\n",
        "            'medications': r'(?:prescribed|medication|drug|med)[:\\s]*([A-Za-z]+(?:\\s+\\d+(?:mg|mcg|g|ml))?)',\n",
        "            'diagnoses': r'(?:diagnosis|diagnosed|condition|assessment)[:\\s]*([A-Za-z\\s]+)',\n",
        "            'symptoms': r'(?:symptom|complaint|presents with|reports)[:\\s]*([A-Za-z\\s]+)',\n",
        "            'procedures': r'(?:procedure|treatment|surgery|operation)[:\\s]*([A-Za-z\\s]+)',\n",
        "            'follow_up': r'(?:follow.?up|return|next visit|appointment)[:\\s]*([A-Za-z0-9\\s]+)',\n",
        "            'allergies': r'(?:allerg|adverse reaction)[:\\s]*([A-Za-z\\s]+)',\n",
        "            'family_history': r'(?:family history|fh)[:\\s]*([A-Za-z\\s]+)'\n",
        "        }\n",
        "\n",
        "    def preprocess_text(self, text: str) -> str:\n",
        "        \"\"\"Clean and normalize text for comparison.\"\"\"\n",
        "        # Remove extra whitespace\n",
        "        text = re.sub(r'\\s+', ' ', text.strip())\n",
        "        # Normalize medical abbreviations\n",
        "        medical_abbrevs = {\n",
        "            r'\\bpt\\b': 'patient',\n",
        "            r'\\bpts\\b': 'patients',\n",
        "            r'\\bw/\\b': 'with',\n",
        "            r'\\bw/o\\b': 'without',\n",
        "            r'\\bc/o\\b': 'complains of',\n",
        "            r'\\bs/p\\b': 'status post',\n",
        "            r'\\bh/o\\b': 'history of'\n",
        "        }\n",
        "        for abbrev, full in medical_abbrevs.items():\n",
        "            text = re.sub(abbrev, full, text, flags=re.IGNORECASE)\n",
        "        return text.lower()\n",
        "\n",
        "    def extract_medical_entities(self, text: str) -> Dict[str, List[str]]:\n",
        "        \"\"\"Extract medical entities using pattern matching and NLP.\"\"\"\n",
        "        entities = {}\n",
        "\n",
        "        # Pattern-based extraction\n",
        "        for category, pattern in self.medical_patterns.items():\n",
        "            matches = re.findall(pattern, text, re.IGNORECASE)\n",
        "            entities[category] = [match.strip() for match in matches if match.strip()]\n",
        "\n",
        "        # NLP-based entity extraction\n",
        "        doc = nlp(text)\n",
        "        entities['named_entities'] = [(ent.text, ent.label_) for ent in doc.ents]\n",
        "        entities['medical_terms'] = []\n",
        "\n",
        "        # Common medical terms\n",
        "        medical_keywords = [\n",
        "            'hypertension', 'diabetes', 'pneumonia', 'infection', 'fever',\n",
        "            'pain', 'headache', 'nausea', 'vomiting', 'diarrhea', 'fatigue',\n",
        "            'chest pain', 'shortness of breath', 'cough', 'rash', 'swelling'\n",
        "        ]\n",
        "\n",
        "        for term in medical_keywords:\n",
        "            if term in text.lower():\n",
        "                entities['medical_terms'].append(term)\n",
        "\n",
        "        return entities\n",
        "\n",
        "    def calculate_semantic_similarity(self, text1: str, text2: str) -> float:\n",
        "        \"\"\"Calculate semantic similarity using sentence transformers.\"\"\"\n",
        "        try:\n",
        "            embeddings = model.encode([text1, text2])\n",
        "            similarity = cosine_similarity([embeddings[0]], [embeddings[1]])[0][0]\n",
        "            return float(similarity)\n",
        "        except Exception:\n",
        "            # Fallback to TF-IDF similarity\n",
        "            vectorizer = TfidfVectorizer()\n",
        "            tfidf_matrix = vectorizer.fit_transform([text1, text2])\n",
        "            similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]\n",
        "            return float(similarity)\n",
        "\n",
        "    def calculate_bleu_score(self, reference: str, candidate: str) -> float:\n",
        "        \"\"\"Calculate BLEU score for text similarity.\"\"\"\n",
        "        def get_ngrams(text: str, n: int) -> List[Tuple]:\n",
        "            words = text.split()\n",
        "            return [tuple(words[i:i+n]) for i in range(len(words)-n+1)]\n",
        "\n",
        "        ref_words = reference.split()\n",
        "        cand_words = candidate.split()\n",
        "\n",
        "        if len(cand_words) == 0:\n",
        "            return 0.0\n",
        "\n",
        "        # Calculate precision for 1-4 grams\n",
        "        precisions = []\n",
        "        for n in range(1, 5):\n",
        "            ref_ngrams = get_ngrams(reference, n)\n",
        "            cand_ngrams = get_ngrams(candidate, n)\n",
        "\n",
        "            if not cand_ngrams:\n",
        "                precisions.append(0.0)\n",
        "                continue\n",
        "\n",
        "            ref_counter = Counter(ref_ngrams)\n",
        "            cand_counter = Counter(cand_ngrams)\n",
        "\n",
        "            overlap = sum((ref_counter & cand_counter).values())\n",
        "            precision = overlap / len(cand_ngrams)\n",
        "            precisions.append(precision)\n",
        "\n",
        "        # Geometric mean of precisions\n",
        "        if any(p == 0 for p in precisions):\n",
        "            return 0.0\n",
        "\n",
        "        bleu = np.exp(np.mean(np.log(precisions)))\n",
        "\n",
        "        # Brevity penalty\n",
        "        bp = min(1.0, np.exp(1 - len(ref_words) / len(cand_words)))\n",
        "\n",
        "        return bleu * bp\n",
        "\n",
        "    def calculate_rouge_l(self, reference: str, candidate: str) -> float:\n",
        "        \"\"\"Calculate ROUGE-L score based on longest common subsequence.\"\"\"\n",
        "        def lcs_length(x: List[str], y: List[str]) -> int:\n",
        "            m, n = len(x), len(y)\n",
        "            dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
        "\n",
        "            for i in range(1, m + 1):\n",
        "                for j in range(1, n + 1):\n",
        "                    if x[i-1] == y[j-1]:\n",
        "                        dp[i][j] = dp[i-1][j-1] + 1\n",
        "                    else:\n",
        "                        dp[i][j] = max(dp[i-1][j], dp[i][j-1])\n",
        "\n",
        "            return dp[m][n]\n",
        "\n",
        "        ref_words = reference.split()\n",
        "        cand_words = candidate.split()\n",
        "\n",
        "        if not ref_words or not cand_words:\n",
        "            return 0.0\n",
        "\n",
        "        lcs_len = lcs_length(ref_words, cand_words)\n",
        "\n",
        "        if lcs_len == 0:\n",
        "            return 0.0\n",
        "\n",
        "        precision = lcs_len / len(cand_words)\n",
        "        recall = lcs_len / len(ref_words)\n",
        "\n",
        "        if precision + recall == 0:\n",
        "            return 0.0\n",
        "\n",
        "        f1_score = 2 * precision * recall / (precision + recall)\n",
        "        return f1_score\n",
        "\n",
        "    def calculate_jaccard_similarity(self, text1: str, text2: str) -> float:\n",
        "        \"\"\"Calculate Jaccard similarity coefficient.\"\"\"\n",
        "        words1 = set(text1.split())\n",
        "        words2 = set(text2.split())\n",
        "\n",
        "        intersection = len(words1.intersection(words2))\n",
        "        union = len(words1.union(words2))\n",
        "\n",
        "        return intersection / union if union > 0 else 0.0\n",
        "\n",
        "    def calculate_medical_entity_overlap(self, entities1: Dict, entities2: Dict) -> float:\n",
        "        \"\"\"Calculate overlap of extracted medical entities.\"\"\"\n",
        "        total_overlap = 0\n",
        "        total_entities = 0\n",
        "\n",
        "        for category in self.medical_patterns.keys():\n",
        "            set1 = set(entities1.get(category, []))\n",
        "            set2 = set(entities2.get(category, []))\n",
        "\n",
        "            if set1 or set2:\n",
        "                intersection = len(set1.intersection(set2))\n",
        "                union = len(set1.union(set2))\n",
        "                overlap = intersection / union if union > 0 else 0\n",
        "                total_overlap += overlap\n",
        "                total_entities += 1\n",
        "\n",
        "        return total_overlap / total_entities if total_entities > 0 else 0.0\n",
        "\n",
        "    def assess_factual_consistency(self, text1: str, text2: str, entities1: Dict, entities2: Dict) -> float:\n",
        "        \"\"\"Assess factual consistency between documents.\"\"\"\n",
        "        consistency_score = 0.0\n",
        "        checks = 0\n",
        "\n",
        "        # Check vital signs consistency\n",
        "        vitals1 = entities1.get('vital_signs', [])\n",
        "        vitals2 = entities2.get('vital_signs', [])\n",
        "        if vitals1 and vitals2:\n",
        "            vital_overlap = len(set(vitals1).intersection(set(vitals2))) / len(set(vitals1).union(set(vitals2)))\n",
        "            consistency_score += vital_overlap\n",
        "            checks += 1\n",
        "\n",
        "        # Check medication consistency\n",
        "        meds1 = set(entities1.get('medications', []))\n",
        "        meds2 = set(entities2.get('medications', []))\n",
        "        if meds1 or meds2:\n",
        "            med_overlap = len(meds1.intersection(meds2)) / len(meds1.union(meds2)) if meds1.union(meds2) else 1.0\n",
        "            consistency_score += med_overlap\n",
        "            checks += 1\n",
        "\n",
        "        # Check diagnosis consistency\n",
        "        diag1 = set(entities1.get('diagnoses', []))\n",
        "        diag2 = set(entities2.get('diagnoses', []))\n",
        "        if diag1 or diag2:\n",
        "            diag_overlap = len(diag1.intersection(diag2)) / len(diag1.union(diag2)) if diag1.union(diag2) else 1.0\n",
        "            consistency_score += diag_overlap\n",
        "            checks += 1\n",
        "\n",
        "        return consistency_score / checks if checks > 0 else 0.5\n",
        "\n",
        "    def assess_completeness(self, text1: str, text2: str, entities1: Dict, entities2: Dict) -> float:\n",
        "        \"\"\"Assess completeness of information coverage.\"\"\"\n",
        "        essential_categories = ['vital_signs', 'medications', 'diagnoses', 'symptoms', 'follow_up']\n",
        "\n",
        "        coverage1 = sum(1 for cat in essential_categories if entities1.get(cat))\n",
        "        coverage2 = sum(1 for cat in essential_categories if entities2.get(cat))\n",
        "\n",
        "        # Both should ideally cover similar essential categories\n",
        "        min_coverage = min(coverage1, coverage2)\n",
        "        max_coverage = max(coverage1, coverage2)\n",
        "\n",
        "        if max_coverage == 0:\n",
        "            return 1.0  # Both empty\n",
        "\n",
        "        return min_coverage / max_coverage\n",
        "\n",
        "    def assess_clinical_relevance(self, text1: str, text2: str) -> float:\n",
        "        \"\"\"Assess clinical relevance and medical appropriateness.\"\"\"\n",
        "        clinical_keywords = [\n",
        "            'patient', 'diagnosis', 'treatment', 'medication', 'symptom',\n",
        "            'examination', 'assessment', 'plan', 'history', 'vital',\n",
        "            'laboratory', 'imaging', 'follow-up', 'prognosis', 'therapy'\n",
        "        ]\n",
        "\n",
        "        def count_clinical_terms(text: str) -> int:\n",
        "            count = 0\n",
        "            for keyword in clinical_keywords:\n",
        "                count += len(re.findall(r'\\b' + keyword + r'\\b', text, re.IGNORECASE))\n",
        "            return count\n",
        "\n",
        "        count1 = count_clinical_terms(text1)\n",
        "        count2 = count_clinical_terms(text2)\n",
        "\n",
        "        # Both should have similar clinical term density\n",
        "        total_words1 = len(text1.split())\n",
        "        total_words2 = len(text2.split())\n",
        "\n",
        "        if total_words1 == 0 or total_words2 == 0:\n",
        "            return 0.0\n",
        "\n",
        "        density1 = count1 / total_words1\n",
        "        density2 = count2 / total_words2\n",
        "\n",
        "        # Similarity in clinical term density\n",
        "        avg_density = (density1 + density2) / 2\n",
        "        density_diff = abs(density1 - density2)\n",
        "\n",
        "        return max(0.0, 1.0 - (density_diff / max(avg_density, 0.1)))\n",
        "\n",
        "    def assess_coherence(self, text1: str, text2: str) -> float:\n",
        "        \"\"\"Assess logical flow and coherence of medical notes.\"\"\"\n",
        "        # Check for proper medical note structure\n",
        "        structure_patterns = [\n",
        "            r'(?:chief complaint|cc|presenting complaint)',\n",
        "            r'(?:history of present illness|hpi)',\n",
        "            r'(?:physical examination|exam|pe)',\n",
        "            r'(?:assessment|impression|diagnosis)',\n",
        "            r'(?:plan|treatment|management)'\n",
        "        ]\n",
        "\n",
        "        def has_medical_structure(text: str) -> float:\n",
        "            found_sections = 0\n",
        "            for pattern in structure_patterns:\n",
        "                if re.search(pattern, text, re.IGNORECASE):\n",
        "                    found_sections += 1\n",
        "            return found_sections / len(structure_patterns)\n",
        "\n",
        "        structure1 = has_medical_structure(text1)\n",
        "        structure2 = has_medical_structure(text2)\n",
        "\n",
        "        # Both should have similar structural completeness\n",
        "        avg_structure = (structure1 + structure2) / 2\n",
        "        structure_diff = abs(structure1 - structure2)\n",
        "\n",
        "        coherence_score = avg_structure * (1 - structure_diff)\n",
        "\n",
        "        return min(1.0, coherence_score)\n",
        "\n",
        "    def extract_numerical_values(self, text: str) -> List[str]:\n",
        "        \"\"\"Extract numerical values that might be medically significant.\"\"\"\n",
        "        # Pattern for numbers with units (vital signs, lab values, etc.)\n",
        "        number_pattern = r'\\d+(?:\\.\\d+)?(?:\\s*(?:mg|mcg|g|ml|mmHg|bpm|°F|°C|%|units?))?'\n",
        "        return re.findall(number_pattern, text, re.IGNORECASE)\n",
        "\n",
        "    def compare_numerical_consistency(self, text1: str, text2: str) -> float:\n",
        "        \"\"\"Compare numerical values for consistency.\"\"\"\n",
        "        nums1 = set(self.extract_numerical_values(text1))\n",
        "        nums2 = set(self.extract_numerical_values(text2))\n",
        "\n",
        "        if not nums1 and not nums2:\n",
        "            return 1.0  # No numerical values in either\n",
        "\n",
        "        if not nums1 or not nums2:\n",
        "            return 0.0  # One has numbers, other doesn't\n",
        "\n",
        "        intersection = len(nums1.intersection(nums2))\n",
        "        union = len(nums1.union(nums2))\n",
        "\n",
        "        return intersection / union\n",
        "\n",
        "    def evaluate_similarity(self, document1: str, document2: str) -> EvaluationResult:\n",
        "        \"\"\"Main evaluation function that combines all metrics.\"\"\"\n",
        "\n",
        "        # Preprocess documents\n",
        "        processed_doc1 = self.preprocess_text(document1)\n",
        "        processed_doc2 = self.preprocess_text(document2)\n",
        "\n",
        "        # Extract medical entities\n",
        "        entities1 = self.extract_medical_entities(processed_doc1)\n",
        "        entities2 = self.extract_medical_entities(processed_doc2)\n",
        "\n",
        "        # Calculate all similarity metrics\n",
        "        semantic_sim = self.calculate_semantic_similarity(processed_doc1, processed_doc2)\n",
        "        bleu = self.calculate_bleu_score(processed_doc1, processed_doc2)\n",
        "        rouge_l = self.calculate_rouge_l(processed_doc1, processed_doc2)\n",
        "        jaccard = self.calculate_jaccard_similarity(processed_doc1, processed_doc2)\n",
        "\n",
        "        # Calculate medical-specific metrics\n",
        "        entity_overlap = self.calculate_medical_entity_overlap(entities1, entities2)\n",
        "        factual_consistency = self.assess_factual_consistency(document1, document2, entities1, entities2)\n",
        "        completeness = self.assess_completeness(document1, document2, entities1, entities2)\n",
        "        clinical_relevance = self.assess_clinical_relevance(document1, document2)\n",
        "        coherence = self.assess_coherence(document1, document2)\n",
        "\n",
        "        # Add numerical consistency check\n",
        "        numerical_consistency = self.compare_numerical_consistency(document1, document2)\n",
        "\n",
        "        # Calculate weighted overall score\n",
        "        weights = {\n",
        "            'factual_consistency': 0.40,\n",
        "            'completeness': 0.25,\n",
        "            'clinical_relevance': 0.20,\n",
        "            'coherence': 0.15\n",
        "        }\n",
        "\n",
        "        overall_score = (\n",
        "            factual_consistency * weights['factual_consistency'] +\n",
        "            completeness * weights['completeness'] +\n",
        "            clinical_relevance * weights['clinical_relevance'] +\n",
        "            coherence * weights['coherence']\n",
        "        )\n",
        "\n",
        "        # Detailed analysis\n",
        "        detailed_analysis = {\n",
        "            'entity_comparison': {\n",
        "                'document1_entities': entities1,\n",
        "                'document2_entities': entities2,\n",
        "                'common_entities': self._find_common_entities(entities1, entities2),\n",
        "                'unique_to_doc1': self._find_unique_entities(entities1, entities2),\n",
        "                'unique_to_doc2': self._find_unique_entities(entities2, entities1)\n",
        "            },\n",
        "            'length_comparison': {\n",
        "                'doc1_length': len(document1.split()),\n",
        "                'doc2_length': len(document2.split()),\n",
        "                'length_ratio': len(document1.split()) / max(len(document2.split()), 1)\n",
        "            },\n",
        "            'numerical_consistency': numerical_consistency,\n",
        "            'similarity_metrics': {\n",
        "                'semantic': semantic_sim,\n",
        "                'bleu': bleu,\n",
        "                'rouge_l': rouge_l,\n",
        "                'jaccard': jaccard\n",
        "            }\n",
        "        }\n",
        "\n",
        "        return EvaluationResult(\n",
        "            semantic_similarity=semantic_sim,\n",
        "            bleu_score=bleu,\n",
        "            rouge_l_score=rouge_l,\n",
        "            jaccard_similarity=jaccard,\n",
        "            medical_entity_overlap=entity_overlap,\n",
        "            factual_consistency_score=factual_consistency,\n",
        "            completeness_score=completeness,\n",
        "            clinical_relevance_score=clinical_relevance,\n",
        "            coherence_score=coherence,\n",
        "            overall_score=overall_score,\n",
        "            detailed_analysis=detailed_analysis\n",
        "        )\n",
        "\n",
        "    def _find_common_entities(self, entities1: Dict, entities2: Dict) -> Dict:\n",
        "        \"\"\"Find entities common to both documents.\"\"\"\n",
        "        common = {}\n",
        "        for category in entities1.keys():\n",
        "            if category in entities2:\n",
        "                set1 = set(entities1[category]) if isinstance(entities1[category], list) else set()\n",
        "                set2 = set(entities2[category]) if isinstance(entities2[category], list) else set()\n",
        "                common[category] = list(set1.intersection(set2))\n",
        "        return common\n",
        "\n",
        "    def _find_unique_entities(self, entities1: Dict, entities2: Dict) -> Dict:\n",
        "        \"\"\"Find entities unique to the first document.\"\"\"\n",
        "        unique = {}\n",
        "        for category in entities1.keys():\n",
        "            if category in entities2:\n",
        "                set1 = set(entities1[category]) if isinstance(entities1[category], list) else set()\n",
        "                set2 = set(entities2[category]) if isinstance(entities2[category], list) else set()\n",
        "                unique[category] = list(set1 - set2)\n",
        "            else:\n",
        "                unique[category] = entities1[category]\n",
        "        return unique\n",
        "\n",
        "    def generate_report(self, result: EvaluationResult, doc1_name: str = \"Document 1\", doc2_name: str = \"Document 2\") -> str:\n",
        "        \"\"\"Generate a comprehensive evaluation report.\"\"\"\n",
        "\n",
        "        def score_interpretation(score: float) -> str:\n",
        "            if score >= 0.9: return \"Excellent\"\n",
        "            elif score >= 0.8: return \"Very Good\"\n",
        "            elif score >= 0.7: return \"Good\"\n",
        "            elif score >= 0.6: return \"Fair\"\n",
        "            elif score >= 0.5: return \"Poor\"\n",
        "            else: return \"Very Poor\"\n",
        "\n",
        "        report = f\"\"\"\n",
        "MEDICAL NOTE SIMILARITY EVALUATION REPORT\n",
        "Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "=========================================\n",
        "\n",
        "EXECUTIVE SUMMARY\n",
        "-----------------\n",
        "Overall Similarity Score: {result.overall_score:.3f} ({score_interpretation(result.overall_score)})\n",
        "\n",
        "The two medical notes show {score_interpretation(result.overall_score).lower()} similarity in conveying the same medical information effectively.\n",
        "\n",
        "DETAILED METRICS\n",
        "----------------\n",
        "\n",
        "1. FACTUAL CONSISTENCY: {result.factual_consistency_score:.3f} ({score_interpretation(result.factual_consistency_score)})\n",
        "   - Measures agreement on medical facts (diagnoses, medications, vital signs)\n",
        "\n",
        "2. COMPLETENESS: {result.completeness_score:.3f} ({score_interpretation(result.completeness_score)})\n",
        "   - Evaluates coverage of essential medical information categories\n",
        "\n",
        "3. CLINICAL RELEVANCE: {result.clinical_relevance_score:.3f} ({score_interpretation(result.clinical_relevance_score)})\n",
        "   - Assesses appropriate use of medical terminology and concepts\n",
        "\n",
        "4. COHERENCE: {result.coherence_score:.3f} ({score_interpretation(result.coherence_score)})\n",
        "   - Evaluates logical structure and medical note organization\n",
        "\n",
        "SIMILARITY METRICS\n",
        "------------------\n",
        "• Semantic Similarity (AI-based): {result.semantic_similarity:.3f}\n",
        "• BLEU Score (n-gram overlap): {result.bleu_score:.3f}\n",
        "• ROUGE-L Score (sequence similarity): {result.rouge_l_score:.3f}\n",
        "• Jaccard Similarity (word overlap): {result.jaccard_similarity:.3f}\n",
        "• Medical Entity Overlap: {result.medical_entity_overlap:.3f}\n",
        "\n",
        "DOCUMENT ANALYSIS\n",
        "-----------------\n",
        "Length Comparison:\n",
        "• {doc1_name}: {result.detailed_analysis['length_comparison']['doc1_length']} words\n",
        "• {doc2_name}: {result.detailed_analysis['length_comparison']['doc2_length']} words\n",
        "• Length Ratio: {result.detailed_analysis['length_comparison']['length_ratio']:.2f}\n",
        "\n",
        "Numerical Consistency: {result.detailed_analysis['numerical_consistency']:.3f}\n",
        "\n",
        "ENTITY ANALYSIS\n",
        "---------------\n",
        "\"\"\"\n",
        "\n",
        "        # Add common entities\n",
        "        common_entities = result.detailed_analysis['entity_comparison']['common_entities']\n",
        "        if any(common_entities.values()):\n",
        "            report += \"\\nCommon Medical Entities:\\n\"\n",
        "            for category, entities in common_entities.items():\n",
        "                if entities:\n",
        "                    # Handle list of tuples for named_entities\n",
        "                    if category == 'named_entities':\n",
        "                        entity_strings = [f\"{text} ({label})\" for text, label in entities]\n",
        "                        report += f\"• {category.replace('_', ' ').title()}: {', '.join(entity_strings)}\\n\"\n",
        "                    else:\n",
        "                        report += f\"• {category.replace('_', ' ').title()}: {', '.join(entities)}\\n\"\n",
        "\n",
        "        # Add unique entities\n",
        "        unique1 = result.detailed_analysis['entity_comparison']['unique_to_doc1']\n",
        "        unique2 = result.detailed_analysis['entity_comparison']['unique_to_doc2']\n",
        "\n",
        "        if any(entities for entities in unique1.values() if entities):\n",
        "            report += f\"\\nUnique to {doc1_name}:\\n\"\n",
        "            for category, entities in unique1.items():\n",
        "                if entities:\n",
        "                     # Handle list of tuples for named_entities\n",
        "                    if category == 'named_entities':\n",
        "                        entity_strings = [f\"{text} ({label})\" for text, label in entities]\n",
        "                        report += f\"• {category.replace('_', ' ').title()}: {', '.join(entity_strings)}\\n\"\n",
        "                    else:\n",
        "                        report += f\"• {category.replace('_', ' ').title()}: {', '.join(entities)}\\n\"\n",
        "\n",
        "        if any(entities for entities in unique2.values() if entities):\n",
        "            report += f\"\\nUnique to {doc2_name}:\\n\"\n",
        "            for category, entities in unique2.items():\n",
        "                if entities:\n",
        "                     # Handle list of tuples for named_entities\n",
        "                    if category == 'named_entities':\n",
        "                        entity_strings = [f\"{text} ({label})\" for text, label in entities]\n",
        "                        report += f\"• {category.replace('_', ' ').title()}: {', '.join(entity_strings)}\\n\"\n",
        "                    else:\n",
        "                        report += f\"• {category.replace('_', ' ').title()}: {', '.join(entities)}\\n\"\n",
        "\n",
        "        # Recommendations\n",
        "        report += \"\\nRECOMMENDATIONS\\n---------------\\n\"\n",
        "\n",
        "        if result.overall_score >= 0.8:\n",
        "            report += \"✓ The documents show high similarity and likely convey the same medical message effectively.\\n\"\n",
        "        elif result.overall_score >= 0.6:\n",
        "            report += \"⚠ The documents show moderate similarity. Review differences in key medical entities.\\n\"\n",
        "        else:\n",
        "            report += \"⚠ The documents show low similarity. Significant differences detected that may affect medical accuracy.\\n\"\n",
        "\n",
        "        if result.factual_consistency_score < 0.7:\n",
        "            report += \"• Review factual inconsistencies in medical data (vital signs, medications, diagnoses)\\n\"\n",
        "\n",
        "        if result.completeness_score < 0.7:\n",
        "            report += \"• One document may be missing essential medical information categories\\n\"\n",
        "\n",
        "        if result.clinical_relevance_score < 0.7:\n",
        "            report += \"• Check for appropriate use of medical terminology and clinical concepts\\n\"\n",
        "\n",
        "        if result.coherence_score < 0.7:\n",
        "            report += \"• Review document structure and logical flow of medical information\\n\"\n",
        "\n",
        "        return report\n",
        "\n",
        "# Usage Example and Testing Function\n",
        "def evaluate_medical_notes(document1: str, document2: str, doc1_name: str = \"Document 1\", doc2_name: str = \"Document 2\"):\n",
        "    \"\"\"\n",
        "    Main function to evaluate two medical notes.\n",
        "\n",
        "    Args:\n",
        "        document1: First medical note text\n",
        "        document2: Second medical note text\n",
        "        doc1_name: Name/identifier for first document\n",
        "        doc2_name: Name/identifier for second document\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (EvaluationResult, formatted_report)\n",
        "    \"\"\"\n",
        "    evaluator = MedicalNoteEvaluator()\n",
        "    result = evaluator.evaluate_similarity(document1, document2)\n",
        "    report = evaluator.generate_report(result, doc1_name, doc2_name)\n",
        "\n",
        "    return result, report\n",
        "\n",
        "# Example usage and test function\n",
        "def run_example():\n",
        "    \"\"\"Run with sample medical notes to demonstrate functionality.\"\"\"\n",
        "\n",
        "    sample_note1 = \"\"\"\n",
        "    Chief Complaint: Patient presents with chest pain and shortness of breath.\n",
        "\n",
        "    History of Present Illness:\n",
        "    45-year-old male with history of hypertension presents with acute onset chest pain\n",
        "    starting 2 hours ago. Pain is crushing, substernal, radiates to left arm.\n",
        "    Associated with diaphoresis and nausea.\n",
        "\n",
        "    Vital Signs: BP 150/95, HR 105, RR 22, Temp 98.6°F, O2 sat 96%\n",
        "\n",
        "    Physical Examination:\n",
        "    Cardiovascular: Tachycardic, regular rhythm, no murmurs\n",
        "    Pulmonary: Clear to auscultation bilaterally\n",
        "\n",
        "    Assessment: Acute coronary syndrome, rule out myocardial infarction\n",
        "\n",
        "    Plan:\n",
        "    - EKG and cardiac enzymes\n",
        "    - Aspirin 325mg, Nitroglycerin PRN\n",
        "    - Cardiology consultation\n",
        "    - Serial cardiac monitoring\n",
        "    \"\"\"\n",
        "\n",
        "    sample_note2 = \"\"\"\n",
        "    CC: 45 y/o male c/o acute chest pain and SOB\n",
        "\n",
        "    HPI: Patient with HTN history presents with sudden onset crushing chest pain\n",
        "    beginning 2 hours prior to arrival. Pain is substernal with radiation to L arm,\n",
        "    accompanied by sweating and nausea.\n",
        "\n",
        "    Vitals: Blood pressure 150/95 mmHg, pulse 105 bpm, respiratory rate 22,\n",
        "    temperature 98.6 degrees F, oxygen saturation 96%\n",
        "\n",
        "    PE:\n",
        "    CV: Tachycardic rate, regular, no murmurs appreciated\n",
        "    Lungs: CTAB\n",
        "\n",
        "    A&P: Acute coronary syndrome, r/o MI\n",
        "    - Obtain EKG and cardiac biomarkers\n",
        "    - Start ASA 325mg, nitroglycerin as needed\n",
        "    - Cardiology consult\n",
        "    - Continuous cardiac monitoring\n",
        "    \"\"\"\n",
        "\n",
        "    result, report = evaluate_medical_notes(sample_note1, sample_note2, \"LLM Model A\", \"LLM Model B\")\n",
        "\n",
        "    print(\"SAMPLE EVALUATION RESULTS:\")\n",
        "    print(\"=\" * 50)\n",
        "    print(report)\n",
        "\n",
        "    return result, report\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run example to demonstrate functionality\n",
        "    run_example()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"TO USE WITH YOUR DOCUMENTS:\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"\"\"\n",
        "# Load your documents\n",
        "with open('note1.txt', 'r') as f:\n",
        "    doc1 = f.read()\n",
        "\n",
        "with open('note2.txt', 'r') as f:\n",
        "    doc2 = f.read()\n",
        "\n",
        "# Evaluate similarity\n",
        "result, report = evaluate_medical_notes(doc1, doc2, \"Model A Output\", \"Model B Output\")\n",
        "\n",
        "# Print results\n",
        "print(report)\n",
        "\n",
        "# Access individual metrics\n",
        "print(f\"Overall Score: {result.overall_score}\")\n",
        "print(f\"Semantic Similarity: {result.semantic_similarity}\")\n",
        "print(f\"Medical Entity Overlap: {result.medical_entity_overlap}\")\n",
        "\"\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w6g1WGrZZdSb",
        "outputId": "206cb715-eac4-4c62-9d5d-53da746e35d0"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py:1750: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SAMPLE EVALUATION RESULTS:\n",
            "==================================================\n",
            "\n",
            "MEDICAL NOTE SIMILARITY EVALUATION REPORT\n",
            "Generated: 2025-08-08 06:58:14\n",
            "=========================================\n",
            "\n",
            "EXECUTIVE SUMMARY\n",
            "-----------------\n",
            "Overall Similarity Score: 0.455 (Very Poor)\n",
            "\n",
            "The two medical notes show very poor similarity in conveying the same medical information effectively.\n",
            "\n",
            "DETAILED METRICS\n",
            "----------------\n",
            "\n",
            "1. FACTUAL CONSISTENCY: 0.300 (Very Poor)\n",
            "   - Measures agreement on medical facts (diagnoses, medications, vital signs)\n",
            "\n",
            "2. COMPLETENESS: 0.667 (Fair)\n",
            "   - Evaluates coverage of essential medical information categories\n",
            "\n",
            "3. CLINICAL RELEVANCE: 0.481 (Very Poor)\n",
            "   - Assesses appropriate use of medical terminology and concepts\n",
            "\n",
            "4. COHERENCE: 0.480 (Very Poor)\n",
            "   - Evaluates logical structure and medical note organization\n",
            "\n",
            "SIMILARITY METRICS\n",
            "------------------\n",
            "• Semantic Similarity (AI-based): 0.772\n",
            "• BLEU Score (n-gram overlap): 0.000\n",
            "• ROUGE-L Score (sequence similarity): 0.375\n",
            "• Jaccard Similarity (word overlap): 0.220\n",
            "• Medical Entity Overlap: 0.200\n",
            "\n",
            "DOCUMENT ANALYSIS\n",
            "-----------------\n",
            "Length Comparison:\n",
            "• LLM Model A: 96 words\n",
            "• LLM Model B: 95 words\n",
            "• Length Ratio: 1.01\n",
            "\n",
            "Numerical Consistency: 0.500\n",
            "\n",
            "ENTITY ANALYSIS\n",
            "---------------\n",
            "\n",
            "Common Medical Entities:\n",
            "• Vital Signs: 98, 150/95, 96\n",
            "• Named Entities: 96% (PERCENT), 325 (CARDINAL), 22 (CARDINAL), 105 (CARDINAL)\n",
            "• Medical Terms: nausea, pain, chest pain\n",
            "\n",
            "Unique to LLM Model A:\n",
            "• Diagnoses: acute coronary syndrome\n",
            "• Symptoms: patient presents with chest pain and shortness of breath, acute onset chest pain starting\n",
            "• Named Entities: 2 hours ago (TIME), 150/95 (DATE), 45-year-old (DATE), 98.6°f (CARDINAL)\n",
            "• Medical Terms: shortness of breath, hypertension\n",
            "\n",
            "Unique to LLM Model B:\n",
            "• Vital Signs: 105, 22\n",
            "• Symptoms: sudden onset crushing chest pain beginning\n",
            "• Named Entities: 2 hours (TIME), 45 (CARDINAL), nitroglycerin (TIME), 98.6 degrees (QUANTITY), 150/95 (CARDINAL), a&p (PRODUCT)\n",
            "\n",
            "RECOMMENDATIONS\n",
            "---------------\n",
            "⚠ The documents show low similarity. Significant differences detected that may affect medical accuracy.\n",
            "• Review factual inconsistencies in medical data (vital signs, medications, diagnoses)\n",
            "• One document may be missing essential medical information categories\n",
            "• Check for appropriate use of medical terminology and clinical concepts\n",
            "• Review document structure and logical flow of medical information\n",
            "\n",
            "\n",
            "==================================================\n",
            "TO USE WITH YOUR DOCUMENTS:\n",
            "==================================================\n",
            "\n",
            "# Load your documents\n",
            "with open('note1.txt', 'r') as f:\n",
            "    doc1 = f.read()\n",
            "\n",
            "with open('note2.txt', 'r') as f:\n",
            "    doc2 = f.read()\n",
            "\n",
            "# Evaluate similarity\n",
            "result, report = evaluate_medical_notes(doc1, doc2, \"Model A Output\", \"Model B Output\")\n",
            "\n",
            "# Print results\n",
            "print(report)\n",
            "\n",
            "# Access individual metrics\n",
            "print(f\"Overall Score: {result.overall_score}\")\n",
            "print(f\"Semantic Similarity: {result.semantic_similarity}\")\n",
            "print(f\"Medical Entity Overlap: {result.medical_entity_overlap}\")\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XTfbMkAWc9mv"
      },
      "execution_count": 9,
      "outputs": []
    }
  ]
}